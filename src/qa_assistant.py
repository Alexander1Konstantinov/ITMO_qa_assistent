import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.utilities import WikipediaAPIWrapper
from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent
from langchain import hub
from langchain_mistralai import ChatMistralAI
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOpenAI
from langchain.agents import AgentType
from langchain.chains import LLMMathChain
from langchain.memory import ConversationBufferMemory


# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv(".env")

class RAGAssistant:
    def __init__(self, docs_folder="parsed_pages", vector_db_path="vector_db"):
        self.docs_folder = docs_folder
        self.vector_db_path = vector_db_path
        self.vector_db = None
        self.agent = None
        self.embeddings = None
        self.llm = None

        if not os.path.exists(self.docs_folder):
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ '{self.docs_folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._setup_embeddings()
        self._setup_llm()
        self._setup_vector_db()
        self._setup_agent()  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –≤–º–µ—Å—Ç–æ QA-—Ü–µ–ø–∏

    def _setup_embeddings(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={"device": "cpu"},
        )
        print("üîç –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def _setup_llm(self):
        api_key = os.getenv("MISTRAL_KEY")
        self.llm = ChatMistralAI(
            model="mistral-medium-2505", temperature=0, max_retries=2, api_key=api_key
        )
        print("üí¨ –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def _load_documents(self):
        documents = []
        for filename in os.listdir(self.docs_folder):
            if filename.endswith(".txt"):
                filepath = os.path.join(self.docs_folder, filename)
                try:
                    loader = TextLoader(filepath, encoding="utf-8")
                    docs = loader.load()
                    documents.extend(docs)
                    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {filename}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {str(e)}")
        return documents

    def _setup_vector_db(self):
        if os.path.exists(self.vector_db_path):
            try:
                print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∏–∑ {self.vector_db_path}")
                self.vector_db = FAISS.load_local(
                    self.vector_db_path,
                    self.embeddings,
                    allow_dangerous_deserialization=True,
                )
                return
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {str(e)}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –±–∞–∑—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")
        documents = self._load_documents()
        if not documents:
            raise ValueError("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        chunks = text_splitter.split_documents(documents)
        print(f"üìö –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞")

        self.vector_db = FAISS.from_documents(chunks, self.embeddings)
        self.vector_db.save_local(self.vector_db_path)
        print(f"üíæ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.vector_db_path}")

    def _setup_tools(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–∞"""
        tools = []
        
        # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 1: –ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (RAG)
        def document_search(query: str) -> str:
            """–ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
            docs = self.vector_db.similarity_search(query, k=2)
            return "\n\n".join([f"üìÑ {os.path.basename(doc.metadata['source'])}:\n{doc.page_content[:300]}..." for doc in docs])
        
        tools.append(Tool(
            name="DocumentSearch",
            func=document_search,
            description="–ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏ —Ñ–∞–π–ª–∞—Ö"
        ))
        
        # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 2: –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä
        math_chain = LLMMathChain.from_llm(llm=self.llm)
        tools.append(Tool(
            name="Calculator",
            func=math_chain.run,
            description="–ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—á–µ—Ç–æ–≤ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"
        ))
        
        # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 3: –ü–æ–∏—Å–∫ –≤ Wikipedia
        wikipedia = WikipediaAPIWrapper()
        tools.append(Tool(
            name="WikipediaSearch",
            func=wikipedia.run,
            description="–ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ Wikipedia"
        ))
        
        return tools

    def _setup_agent(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∏ –ø–∞–º—è—Ç—å—é"""
        tools = self._setup_tools()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = hub.pull("hwchase17/react-chat")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–º—è—Ç—å –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # –°–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞–º—è—Ç–∏
        agent = create_react_agent(
            llm=self.llm,
            tools=tools,
            prompt=prompt
        )
        
        self.agent = AgentExecutor(
            agent=agent,
            tools=tools,
            memory=memory,  # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–º—è—Ç—å
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=5
        )
        print("ü§ñ –ê–≥–µ–Ω—Ç —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∏ –ø–∞–º—è—Ç—å—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def ask(self, query):
        """
        –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∞–≥–µ–Ω—Ç—É
        
        :param query: –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞
        :return: –û—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        """
        try:
            # –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ invoke –≤–º–µ—Å—Ç–æ run
            result = self.agent.invoke({"input": query})
            
            return {
                "answer": result["output"],
                "sources": self._extract_sources(result)
            }
        except Exception as e:
            return {"error": str(e)}
    
    def _extract_sources(self, result):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –ª–æ–≥–∞ –∞–≥–µ–Ω—Ç–∞"""
        sources = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —à–∞–≥–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞
        for step in result.get("intermediate_steps", []):
            # –®–∞–≥ —Å–æ–¥–µ—Ä–∂–∏—Ç (–¥–µ–π—Å—Ç–≤–∏–µ, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞–±–ª—é–¥–µ–Ω–∏—è)
            action, observation = step
            
            # –î–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if action.tool == "DocumentSearch":
                # –†–∞–∑–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
                for part in observation.split("\n\n"):
                    if part.startswith("üìÑ"):
                        source, content = part.split(":", 1)
                        sources.append({
                            "source": source.strip("üìÑ "),
                            "content": content.strip()[:200] + "..."
                        })
        
        return sources

    def interactive_mode(self):
        print("\n" + "=" * 50)
        print("ü§ñ RAG-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ (–ê–≥–µ–Ω—Ç—Å–∫–∏–π —Ä–µ–∂–∏–º)")
        print("=" * 50 + "\n")

        while True:
            query = input("‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
            if query.lower() in ['exit', '–≤—ã—Ö–æ–¥']:
                break

            response = self.ask(query)

            if "error" in response:
                print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞: {response['error']}\n")
            else:
                print(f"\nüí° –û—Ç–≤–µ—Ç: {response['answer']}")

                if response["sources"]:
                    print("\nüîç –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:")
                    for i, source in enumerate(response["sources"], 1):
                        print(f"{i}. –§–∞–π–ª: {source['source']}")
                        print(f"   –§—Ä–∞–≥–º–µ–Ω—Ç: {source['content']}")

                print("\n" + "-" * 50)


def main():
    try:
        assistant = RAGAssistant(docs_folder="parsed_pages", vector_db_path="vector_db")
        assistant.interactive_mode()

    except Exception as e:
        print(f"\nüõë –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


if __name__ == "__main__":
    print("–ó–∞–ø—É—Å–∫ RAG-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ –∞–≥–µ–Ω—Ç—Å–∫–æ–º —Ä–µ–∂–∏–º–µ...")
    main()